# ğŸ”„ Drop-in Integrations Quick Start

Replace your existing AI provider APIs with zero code changes. Get instant fallbacks, load balancing, and cost optimization.

## ğŸ¯ What You'll Get

- âœ… **Zero code changes** - Just update your base URL
- âœ… **Instant fallbacks** - Automatic provider redundancy
- âœ… **Better reliability** - Built-in retry and error handling
- âœ… **Cost optimization** - Key rotation and usage balancing
- âœ… **Observability** - Built-in metrics and monitoring

---

## âš¡ Prerequisites

- âœ… Existing application using OpenAI, Anthropic, or other supported providers
- âœ… Bifrost HTTP server running ([30-second setup](http-transport.md))
- âœ… Your existing API keys

---

## ğŸš€ Choose Your Integration

### ğŸ¤– OpenAI â†’ Bifrost

**Before:**

```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY"
```

**After:**

```bash
curl http://localhost:8080/openai/chat/completions \
  # No authorization header needed - keys in config
```

### ğŸ§  Anthropic â†’ Bifrost

**Before:**

```bash
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: YOUR_API_KEY"
```

**After:**

```bash
curl http://localhost:8080/anthropic/v1/messages \
  # No API key header needed - keys in config
```

### ğŸ” Google GenAI â†’ Bifrost

**Before:**

```bash
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent \
  -H "Authorization: Bearer YOUR_API_KEY"
```

**After:**

```bash
curl http://localhost:8080/genai/v1beta/models/gemini-pro:generateContent \
  # No authorization header needed - keys in config
```

---

## ğŸ”§ Setup (1 minute)

### Step 1: Configure Bifrost

Create `config.json` with your providers:

```json
{
  "providers": {
    "openai": {
      "keys": [
        {
          "value": "env.OPENAI_API_KEY",
          "models": ["gpt-4o-mini", "gpt-4o"],
          "weight": 1.0
        }
      ]
    },
    "anthropic": {
      "keys": [
        {
          "value": "env.ANTHROPIC_API_KEY",
          "models": ["claude-3-sonnet-20240229"],
          "weight": 1.0
        }
      ]
    }
  }
}
```

### Step 2: Start Bifrost

```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"

docker run -p 8080:8080 \
  -v $(pwd)/config.json:/app/config/config.json \
  -e OPENAI_API_KEY \
  -e ANTHROPIC_API_KEY \
  maximhq/bifrost
```

### Step 3: Update Your Application

<details open>
<summary><strong>ğŸ Python Applications</strong></summary>

```python
# Before
from openai import OpenAI
client = OpenAI(api_key="your-key")

# After - Just change base_url!
from openai import OpenAI
client = OpenAI(
    api_key="dummy",  # Not used, but required by SDK
    base_url="http://localhost:8080/openai"
)

# Your existing code works unchanged!
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

</details>

<details>
<summary><strong>ğŸŸ¨ JavaScript/Node.js Applications</strong></summary>

```javascript
// Before
import OpenAI from "openai";
const client = new OpenAI({ apiKey: "your-key" });

// After - Just change baseURL!
import OpenAI from "openai";
const client = new OpenAI({
  apiKey: "dummy", // Not used, but required by SDK
  baseURL: "http://localhost:8080/openai",
});

// Your existing code works unchanged!
const response = await client.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Hello!" }],
});
```

</details>

<details>
<summary><strong>ğŸ¦€ Rust Applications</strong></summary>

```rust
// Before
let client = async_openai::Client::new()
    .with_api_key("your-key");

// After - Just change base_url!
let client = async_openai::Client::new()
    .with_api_key("dummy")  // Not used, but required
    .with_api_base("http://localhost:8080/openai");

// Your existing code works unchanged!
let request = CreateChatCompletionRequestArgs::default()
    .model("gpt-4o-mini")
    .messages(messages)
    .build()?;
```

</details>

<details>
<summary><strong>ğŸ”¤ cURL/REST Applications</strong></summary>

```bash
# Before
curl https://api.openai.com/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4o-mini", "messages": [...]}'

# After - Just change URL!
curl http://localhost:8080/openai/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4o-mini", "messages": [...]}'
  # No Authorization header needed!
```

</details>

---

## ğŸ¯ You Did It!

ğŸ‰ **Congratulations!** You've successfully:

- âœ… **Zero-downtime migration** - No application changes needed
- âœ… **Enhanced reliability** - Built-in error handling and retries
- âœ… **Provider flexibility** - Can switch providers transparently
- âœ… **Better observability** - Metrics and monitoring included

---

## ğŸš€ Power Features (5 minutes each)

### Add Automatic Fallbacks

Your applications automatically get fallbacks! Configure in Bifrost:

```json
{
  "providers": {
    "openai": {
      "keys": [
        {
          "value": "env.OPENAI_API_KEY",
          "models": ["gpt-4o-mini"],
          "weight": 1.0
        }
      ]
    },
    "anthropic": {
      "keys": [
        {
          "value": "env.ANTHROPIC_API_KEY",
          "models": ["claude-3-sonnet-20240229"],
          "weight": 1.0
        }
      ]
    }
  }
}
```

Now make requests with automatic fallbacks:

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}],
    "fallbacks": [
      {"provider": "anthropic", "model": "claude-3-sonnet-20240229"}
    ]
  }'
```

### Multiple API Keys per Provider

Load balance and rotate keys automatically:

```json
{
  "providers": {
    "openai": {
      "keys": [
        {
          "value": "env.OPENAI_API_KEY_1",
          "models": ["gpt-4o-mini"],
          "weight": 0.7
        },
        {
          "value": "env.OPENAI_API_KEY_2",
          "models": ["gpt-4o-mini"],
          "weight": 0.3
        }
      ]
    }
  }
}
```

### Built-in Monitoring

Access metrics automatically:

```bash
curl http://localhost:8080/metrics
```

Returns Prometheus metrics for:

- Request success/failure rates
- Latency per provider
- Key usage distribution
- Error rates and types

---

## ğŸ”„ Provider-Specific Guides

### ğŸ¤– OpenAI Integration

Your OpenAI applications get these **instant upgrades**:

| Feature          | Before      | After        |
| ---------------- | ----------- | ------------ |
| **Fallbacks**    | âŒ Manual   | âœ… Automatic |
| **Key Rotation** | âŒ Manual   | âœ… Automatic |
| **Retries**      | âŒ SDK only | âœ… Advanced  |
| **Monitoring**   | âŒ Manual   | âœ… Built-in  |

**Compatible Endpoints:**

- âœ… `/openai/chat/completions` - Chat completions
- âœ… `/openai/completions` - Text completions
- âœ… `/openai/embeddings` - Text embeddings

### ğŸ§  Anthropic Integration

Your Anthropic applications get these **instant upgrades**:

| Feature            | Before    | After          |
| ------------------ | --------- | -------------- |
| **Fallbacks**      | âŒ None   | âœ… Automatic   |
| **Vision Support** | âœ… Native | âœ… Enhanced    |
| **Tool Calling**   | âœ… Native | âœ… + MCP Tools |

**Compatible Endpoints:**

- âœ… `/anthropic/v1/messages` - Messages API

### ğŸ” Google GenAI Integration

**Compatible Endpoints:**

- âœ… `/genai/v1beta/models/{model}:generateContent`
- âœ… `/genai/v1beta/models/{model}:streamGenerateContent`

---

## ğŸ’¡ Migration Strategies

<details>
<summary><strong>ğŸ”„ Gradual Migration (Recommended)</strong></summary>

1. **Week 1:** Deploy Bifrost alongside existing setup
2. **Week 2:** Route 10% of traffic through Bifrost
3. **Week 3:** Route 50% of traffic through Bifrost
4. **Week 4:** Route 100% of traffic through Bifrost
5. **Week 5:** Remove old direct provider integration

**Benefits:**

- âœ… Zero risk
- âœ… Performance validation
- âœ… Easy rollback

</details>

<details>
<summary><strong>âš¡ Instant Migration</strong></summary>

1. **Deploy Bifrost** with current provider configuration
2. **Update base URLs** in your applications
3. **Remove API key headers** from requests
4. **Deploy applications** with new configuration

**Benefits:**

- âœ… Immediate benefits
- âœ… Simple process
- âœ… Quick wins

</details>

<details>
<summary><strong>ğŸ§ª Blue-Green Migration</strong></summary>

1. **Set up Bifrost** in staging environment
2. **Test all endpoints** with production traffic patterns
3. **Switch DNS/load balancer** to point to Bifrost
4. **Monitor and validate** performance

**Benefits:**

- âœ… Production validation
- âœ… Instant rollback capability
- âœ… Confidence in migration

</details>

---

## ğŸ“Š Benefits You Get Immediately

### ğŸ›¡ï¸ Reliability Improvements

| Metric             | Before          | After                     |
| ------------------ | --------------- | ------------------------- |
| **Uptime**         | Single provider | Multi-provider redundancy |
| **Error Handling** | SDK-dependent   | Advanced retry logic      |
| **Rate Limits**    | Hard failures   | Automatic key rotation    |

### ğŸ’° Cost Optimization

- **ğŸ”‘ Key Rotation:** Distribute load across multiple keys
- **ğŸ“Š Usage Tracking:** Monitor spend per provider/model
- **âš–ï¸ Load Balancing:** Optimize costs with weighted distribution

### ğŸ“ˆ Observability

- **ğŸ“Š Metrics:** Request rates, latency, error rates
- **ğŸ” Tracing:** Full request lifecycle visibility
- **âš ï¸ Alerting:** Built-in failure detection

---

## ğŸ“š What's Next?

### Advanced Features (10 minutes each)

| Feature                    | Documentation                                     | Value                           |
| -------------------------- | ------------------------------------------------- | ------------------------------- |
| **ğŸ› ï¸ MCP Tools**           | [MCP Integration](../features/mcp-integration.md) | Add external tools to any model |
| **ğŸ”Œ Custom Plugins**      | [Plugin System](../features/plugins.md)           | Custom business logic           |
| **ğŸ“Š Advanced Monitoring** | [Observability](../features/observability.md)     | Comprehensive monitoring        |

### Production Setup

- **[ğŸš€ Production Deployment](../configuration/deployment/production.md)** - Scale for production traffic
- **[ğŸ”’ Security Guide](../configuration/security.md)** - Secure your deployment
- **[âš¡ Performance Tuning](../benchmarks.md)** - Optimize for your workload

---

## â“ Need Help?

- **[ğŸ“± Migration Guides](../guides/migration/)** - Provider-specific migration steps
- **[ğŸ”§ Troubleshooting](../guides/troubleshooting.md)** - Common issues and solutions
- **[â“ FAQ](../guides/faq.md)** - Frequently asked questions
- **[ğŸ’¬ GitHub Discussions](https://github.com/maximhq/bifrost/discussions)** - Community support

---

**Questions about your specific provider?** Check our [provider-specific migration guides](../guides/migration/) for detailed instructions.

Built with â¤ï¸ by [Maxim](https://github.com/maximhq)
